{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Author Attribution Using Spark ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63252cd4b448461fb8a2b92d53588105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1572018831354_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-10-23.us-east-2.compute.internal:20888/proxy/application_1572018831354_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-9-223.us-east-2.compute.internal:8042/node/containerlogs/container_1572018831354_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import shutil\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0dbcc5fff1c471f809230d5d2674f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The method wholeTextFiles reads from a filesystem\n",
    "# s3 is not a filesystem However it has been configured to act as a filesystem in AWS EMR.\n",
    "# if you happen to do the same operation from outside EMR  it will throw an \"Unknown filesystem\" error\n",
    "\n",
    "data = sc.wholeTextFiles(\"s3://aws-big-data-project/input_files/*.txt\",use_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5f9aa9901e4047a3bd906f314e6224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FUNCTIONS TO PERFORM VARIOUS OPERATIONS\n",
    "\n",
    "def parse_text(text):\n",
    "    regex = re.compile(r'[\\ufeff\\n\\r\\t]')\n",
    "    t = regex.sub(\" \", text[1])\n",
    "    q = re.sub(r'\\d', ' ', t)\n",
    "    return (text[0],q)\n",
    "\n",
    "def deep_parse_text(text):\n",
    "    text = text.replace('\\ufeff', ' ')\n",
    "    text = re.sub(r'[-!$%^*&>:;,.?#/@\"()\\[\\]]',' ',text.lower().replace('\\r\\n', ' '))\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    text = re.sub(r'twain|chapter',' ',text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def filter_text(text, lst):\n",
    "    file = text[0]\n",
    "    bg = text[1]\n",
    "    res=[]\n",
    "\n",
    "    for j in bg:\n",
    "        if j in lst:\n",
    "            res.append(j)\n",
    "    return (file,res)\n",
    "\n",
    "def token_to_pos(ch):\n",
    "    tokens = nltk.word_tokenize(ch[1])\n",
    "    return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "\n",
    "def Syn(ch):\n",
    "    chapters_pos = token_to_pos(ch)\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    fvs_syntax = [[ch.count(pos) for pos in pos_list] for ch in chapters_pos]\n",
    "    fvs_syntax = fvs_syntax[0]\n",
    "\n",
    "    result=[]\n",
    "\n",
    "    for i in fvs_syntax:\n",
    "        for j in chapters_pos:\n",
    "            result.append(float(format(float(i/len(j)),'.4f')))\n",
    "\n",
    "    r1=result[0]\n",
    "    r2=result[1]\n",
    "    r3=result[2]\n",
    "    r4=result[3]\n",
    "    r5=result[4]\n",
    "    r6=result[5]\n",
    "\n",
    "    return (ch[0],r1,r2,r3,r4,r5,r6)\n",
    "\n",
    "def LexicalFeatures(chapters):\n",
    "    sentence_tokenizer = nltk.data.load('/usr/share/nltk_data/tokenizers/punkt/english.pickle')\n",
    "    word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    tokens = nltk.word_tokenize(chapters[1].lower())\n",
    "    words = word_tokenizer.tokenize(chapters[1].lower())\n",
    "    sentences = sentence_tokenizer.tokenize(chapters[1])\n",
    "    vocab = set(words)\n",
    "\n",
    "    words_per_sentence = np.array([len(word_tokenizer.tokenize(s)) for s in sentences])\n",
    "    fvs_lexical_0 = format(words_per_sentence.mean(),'.4f')\n",
    "    fvs_lexical_1 = format(words_per_sentence.std(),'.4f')\n",
    "    fvs_lexical_2 = format(len(vocab) / float(len(words)),'.4f')\n",
    "    fvs_punct_0 = format(tokens.count(',') / float(len(sentences)),'.4f')\n",
    "    fvs_punct_1 = format(tokens.count(';') / float(len(sentences)),'.4f')\n",
    "    fvs_punct_2 = format(tokens.count(':') / float(len(sentences)),'.4f')\n",
    "\n",
    "    return (chapters[0],float(fvs_lexical_0), float(fvs_lexical_1),float(fvs_lexical_2),float(fvs_punct_0),float(fvs_punct_1),float(fvs_punct_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f10c950472e40de966c9bf2fca5efb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PARSE THE DATA & PERFORM CLEANING \n",
    "\n",
    "data_parse = data.map(lambda x: (x[0].split(\"/\")[-1],deep_parse_text(x[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3191309046c412a8cd474de291c53e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TOKENIZE THE TEXT, REMOVE STOPWORDS & COUNT THE WORD REPEATITION \n",
    "\n",
    "textDF = spark.createDataFrame(data_parse, [\"label\", \"text\"])\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "wordsDataFrame = tokenizer.transform(textDF)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"stwords\")\n",
    "DF = remover.transform(wordsDataFrame)\n",
    "\n",
    "pair = DF.rdd.flatMap(lambda df: df[3]).map(lambda token: (token,1)).reduceByKey(lambda v1,v2 : v1+v2).sortBy(lambda x: x[1],False)\n",
    "\n",
    "stp = pair.map(lambda x: x[0]).collect()[0:20]\n",
    "\n",
    "\n",
    "filrdd = DF.rdd.map(lambda df: (df[0],df[3])).map(lambda l: filter_text(l,stp))\n",
    "FDF = spark.createDataFrame(filrdd, [\"label\", \"finalword\"])\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"finalword\", outputCol=\"features\")\n",
    "model = cv.fit(FDF)\n",
    "result = model.transform(FDF)\n",
    "finaldata = result.select(\"label\",\"features\")\n",
    "\n",
    "word_df = finaldata.rdd.map(lambda x: Row(Doc_Name=x[0],f1=float(x[1].toArray()[0]),f2=float(x[1].toArray()[1]),f3=float(x[1].toArray()[2]),f4=float(x[1].toArray()[3]),f5=float(x[1].toArray()[4]),f6=float(x[1].toArray()[5]),f7=float(x[1].toArray()[6]),f8=float(x[1].toArray()[7]),f9=float(x[1].toArray()[8]),f10=float(x[1].toArray()[9]),f11=float(x[1].toArray()[10]),f12=float(x[1].toArray()[11]),f13=float(x[1].toArray()[12]),f14=float(x[1].toArray()[13]),f15=float(x[1].toArray()[14]),f16=float(x[1].toArray()[15]),f17=float(x[1].toArray()[16]),f18=float(x[1].toArray()[17]),f19=float(x[1].toArray()[18]),f20=float(x[1].toArray()[19]))).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c296e4cc3b4887bbf97dfc25b51272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE BIGRAMS & COUNT THEM\n",
    "\n",
    "bigram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
    "bigramDF = bigram.transform(wordsDataFrame)\n",
    "\n",
    "bipair = bigramDF.rdd.flatMap(lambda df: df[3]).map(lambda token: (token,1)).reduceByKey(lambda v1,v2 : v1+v2).sortBy(lambda x: x[1],False)\n",
    "bistp = bipair.map(lambda x: x[0]).collect()[0:10]\n",
    "\n",
    "bifilrdd = bigramDF.rdd.map(lambda df: (df[0],df[3])).map(lambda l: filter_text(l,bistp))\n",
    "biFDF = spark.createDataFrame(bifilrdd, [\"label\", \"finalword\"])\n",
    "\n",
    "bicv = CountVectorizer(inputCol=\"finalword\", outputCol=\"features\")\n",
    "bimodel = cv.fit(biFDF)\n",
    "biresult = bimodel.transform(biFDF)\n",
    "bifinaldata = biresult.select(\"label\",\"features\")\n",
    "\n",
    "bigram_df = bifinaldata.rdd.map(lambda x: Row(Doc_Name=x[0],bg1=float(x[1].toArray()[0]),bg2=float(x[1].toArray()[1]),bg3=float(x[1].toArray()[2]),bg4=float(x[1].toArray()[3]),bg5=float(x[1].toArray()[4]),bg6=float(x[1].toArray()[5]),bg7=float(x[1].toArray()[6]),bg8=float(x[1].toArray()[7]),bg9=float(x[1].toArray()[8]),bg10=float(x[1].toArray()[9]))).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7946cb1e7764790a54de839a445b848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE TRIGRAMS & COUNT THEM\n",
    "\n",
    "trigram = NGram(n=3, inputCol=\"words\", outputCol=\"trigrams\")\n",
    "trigramDF = trigram.transform(wordsDataFrame)\n",
    "\n",
    "tripair = trigramDF.rdd.flatMap(lambda df: df[3]).map(lambda token: (token,1)).reduceByKey(lambda v1,v2 : v1+v2).sortBy(lambda x: x[1],False)\n",
    "tristp = tripair.map(lambda x: x[0]).collect()[0:10]\n",
    "\n",
    "trifilrdd = trigramDF.rdd.map(lambda df: (df[0],df[3])).map(lambda l: filter_text(l,tristp))\n",
    "triFDF = spark.createDataFrame(trifilrdd, [\"label\", \"finalword\"])\n",
    "\n",
    "tricv = CountVectorizer(inputCol=\"finalword\", outputCol=\"features\")\n",
    "trimodel = cv.fit(triFDF)\n",
    "triresult = trimodel.transform(triFDF)\n",
    "trifinaldata = triresult.select(\"label\",\"features\")\n",
    "\n",
    "trigram_df = trifinaldata.rdd.map(lambda x: Row(Doc_Name=x[0],tg1=float(x[1].toArray()[0]),tg2=float(x[1].toArray()[1]),tg3=float(x[1].toArray()[2]),tg4=float(x[1].toArray()[3]),tg5=float(x[1].toArray()[4]),tg6=float(x[1].toArray()[5]),tg7=float(x[1].toArray()[6]),tg8=float(x[1].toArray()[7]),tg9=float(x[1].toArray()[8]),tg10=float(x[1].toArray()[9]))).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c06738610a1437191f06cf0219a42c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE 4GRAMS & COUNT THEM\n",
    "\n",
    "qgram = NGram(n=4, inputCol=\"words\", outputCol=\"qgrams\")\n",
    "qgramDF = qgram.transform(wordsDataFrame)\n",
    "\n",
    "qpair = qgramDF.rdd.flatMap(lambda df: df[3]).map(lambda token: (token,1)).reduceByKey(lambda v1,v2 : v1+v2).sortBy(lambda x: x[1],False)\n",
    "qstp = qpair.map(lambda x: x[0]).collect()[0:10]\n",
    "\n",
    "qfilrdd = qgramDF.rdd.map(lambda df: (df[0],df[3])).map(lambda l: filter_text(l,qstp))\n",
    "qFDF = spark.createDataFrame(qfilrdd, [\"label\", \"finalword\"])\n",
    "\n",
    "qcv = CountVectorizer(inputCol=\"finalword\", outputCol=\"features\")\n",
    "qmodel = cv.fit(qFDF)\n",
    "qresult = qmodel.transform(qFDF)\n",
    "qfinaldata = qresult.select(\"label\",\"features\")\n",
    "\n",
    "qgram_df = qfinaldata.rdd.map(lambda x: Row(Doc_Name=x[0],qg1=float(x[1].toArray()[0]),qg2=float(x[1].toArray()[1]),qg3=float(x[1].toArray()[2]),qg4=float(x[1].toArray()[3]),qg5=float(x[1].toArray()[4]),qg6=float(x[1].toArray()[5]),qg7=float(x[1].toArray()[6]),qg8=float(x[1].toArray()[7]),qg9=float(x[1].toArray()[8]),qg10=float(x[1].toArray()[9]))).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c8934ef74940e183fce29932c08833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE LEXICAL & SYNTACTICAL FEATURES\n",
    "\n",
    "data2 = data.map(lambda x: (x[0].split(\"/\")[-1],x[1].lower()))\n",
    "\n",
    "data_new = data2.map(parse_text)\n",
    "\n",
    "lexical=data_new.map(LexicalFeatures)\n",
    "\n",
    "syntactic=data_new.map(Syn)\n",
    "\n",
    "lexical_df=spark.createDataFrame(lexical, ['Doc_Name','wrds_sent_mean', 'wrds_sent_std','Lex_diver','Commas_sent','Semicolon_sent','Colons_sent'])\n",
    "syntactic_df=spark.createDataFrame(syntactic, ['Doc_Name','NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57af035e6310400fbb6054ad766df567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE INDIVIDUAL FEATURE LEVEL DATAFRAMES\n",
    "\n",
    "lexical_df.createOrReplaceTempView(\"lexical_df\")\n",
    "syntactic_df.createOrReplaceTempView(\"syntactic_df\")\n",
    "word_df.createOrReplaceTempView(\"word_df\")\n",
    "bigram_df.createOrReplaceTempView(\"bigram_df\")\n",
    "trigram_df.createOrReplaceTempView(\"trigram_df\")\n",
    "qgram_df.createOrReplaceTempView(\"qgram_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50367b555f74ca4a73fc7e99514e80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# COMBINE ALL THE DATAFRAMES USING SQL\n",
    "\n",
    "final_df=spark.sql(\" SELECT CASE WHEN ld.Doc_Name == 'Austen1.txt' THEN '0' \\\n",
    "                                 WHEN ld.Doc_Name == 'Austen2.txt' THEN '0' \\\n",
    "                                 WHEN ld.Doc_Name == 'Austen3.txt' THEN '0' \\\n",
    "                                 WHEN ld.Doc_Name == 'Austen4.txt' THEN '0' \\\n",
    "                                 WHEN ld.Doc_Name == 'Austen5.txt' THEN '0' \\\n",
    "                                 WHEN ld.Doc_Name == 'Dickens1.txt' THEN '1' \\\n",
    "                                 WHEN ld.Doc_Name == 'Dickens2.txt' THEN '1' \\\n",
    "                                 WHEN ld.Doc_Name == 'Dickens3.txt' THEN '1' \\\n",
    "                                 WHEN ld.Doc_Name == 'Dickens4.txt' THEN '1' \\\n",
    "                                 WHEN ld.Doc_Name == 'Dickens5.txt' THEN '1' \\\n",
    "                                 WHEN ld.Doc_Name == 'Twain1.txt' THEN '2' \\\n",
    "                                 WHEN ld.Doc_Name == 'Twain2.txt' THEN '2' \\\n",
    "                                 WHEN ld.Doc_Name == 'Twain3.txt' THEN '2' \\\n",
    "                                 WHEN ld.Doc_Name == 'Twain4.txt' THEN '2' \\\n",
    "                                 WHEN ld.Doc_Name == 'Twain5.txt' THEN '2' END Author,\\\n",
    "                                 ld.wrds_sent_mean, ld.wrds_sent_std, ld.Lex_diver, ld.Commas_sent, ld.Semicolon_sent, ld.Colons_sent, \\\n",
    "                                 sd.NN, sd.NNP, sd.DT, sd.IN, sd.JJ, sd.NNS, wd.f1, wd.f2,  wd.f3, wd.f4, wd.f5, wd.f6, wd.f7, wd.f8, \\\n",
    "                                 wd.f9, wd.f10, wd.f11, wd.f12, wd.f13, wd.f14, wd.f15, wd.f16, wd.f17, wd.f18, wd.f19, wd.f20, bd.bg1, \\\n",
    "                                 bd.bg2, bd.bg3, bd.bg4, bd.bg5, bd.bg6, bd.bg7, bd.bg8, bd.bg9, bd.bg10, td.tg1, td.tg2, td.tg3, td.tg4, \\\n",
    "                                 td.tg5, td.tg6, td.tg7, td.tg8, td.tg9, td.tg10, qd.qg1, qd.qg2, qd.qg3, qd.qg4, qd.qg5, qd.qg6, qd.qg7, \\\n",
    "                                 qd.qg8, qd.qg9, qd.qg10,ld.Doc_Name \\\n",
    "                      FROM lexical_df ld, syntactic_df sd, word_df wd, bigram_df bd, trigram_df td, qgram_df qd \\\n",
    "                      WHERE ld.Doc_Name = sd.Doc_Name and ld.Doc_Name = wd.Doc_Name \\\n",
    "                        and ld.Doc_Name = bd.Doc_Name and ld.Doc_Name = td.Doc_Name and ld.Doc_Name = qd.Doc_Name \")\n",
    "\n",
    "final_df.createOrReplaceTempView(\"final_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97986d5d8add4f86b15cd53dc1f0a542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024e1282b9b2456299fd2be0457b9b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE TRAINING & TEST DATAFRAMES - SPLIT THE FINAL DATAFRAME INTO 2\n",
    "\n",
    "training_df=spark.sql(\"SELECT Author, wrds_sent_mean, wrds_sent_std, Lex_diver, Commas_sent, Semicolon_sent, Colons_sent, NN, NNP, DT, IN, JJ, NNS, \\\n",
    "                           f1, f2,  f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16, f17, f18, f19, f20, bg1, bg2, bg3, bg4, bg5, \\\n",
    "                           bg6, bg7, bg8, bg9, bg10, tg1, tg2, tg3, tg4, tg5, tg6, tg7, tg8, tg9, tg10, qg1, qg2, qg3, qg4, qg5, qg6, qg7, qg8, \\\n",
    "                           qg9, qg10 FROM final_df WHERE Doc_Name not in ('Austen2.txt', 'Dickens3.txt', 'Twain5.txt')\")\n",
    "\n",
    "test_df=spark.sql(\"SELECT Author, wrds_sent_mean, wrds_sent_std, Lex_diver, Commas_sent, Semicolon_sent, Colons_sent, NN, NNP, DT, IN, JJ, NNS, \\\n",
    "                           f1, f2,  f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16, f17, f18, f19, f20, bg1, bg2, bg3, bg4, bg5, \\\n",
    "                           bg6, bg7, bg8, bg9, bg10, tg1, tg2, tg3, tg4, tg5, tg6, tg7, tg8, tg9, tg10, qg1, qg2, qg3, qg4, qg5, qg6, qg7, qg8, \\\n",
    "                           qg9, qg10 FROM final_df WHERE Doc_Name in ('Austen2.txt', 'Dickens3.txt', 'Twain5.txt')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d13db0ab234dda9b551254302d8add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CONVERT TO RDD\n",
    "\n",
    "training= training_df.rdd.map(lambda row: LabeledPoint(row[0], row[1:]))\n",
    "test= test_df.rdd.map(lambda row: LabeledPoint(row[0], row[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc0802d0cfe4430b7be2fdc0c6cfa62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Logistic Regression : \n",
      "\n",
      "[[4. 0. 0.]\n",
      " [0. 4. 0.]\n",
      " [0. 0. 4.]]\n",
      "\n",
      "Summary Of Statistics For Logistic Regression\n",
      "\n",
      "Total Accuracy  : 1.00\n",
      "\n",
      "Total Precision : 1.00\t Precision 1: 1.00\t Precision 2: 1.00\t Precision 3: 1.00\n",
      "\n",
      "Total Recall    : 1.00\t Recall 1   : 1.00\t Recall 2   : 1.00\t Recall 3   : 1.00\n",
      "\n",
      "Total fMeasure  : 1.00"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "\n",
    "                           #   LOGISTIC REGRESSION   #                           \n",
    "\n",
    "#################################################################################\n",
    "\n",
    "## TRAINING THE MODEL\n",
    "\n",
    "modelLR = LogisticRegressionWithLBFGS.train(training, numClasses=3)\n",
    "predictionAndLabelsLR = training.map(lambda l: (float(modelLR.predict(l.features)), l.label))\n",
    "metricsLR = MulticlassMetrics(predictionAndLabelsLR)\n",
    "\n",
    "print('Confusion Matrix for Logistic Regression : \\n\\n' + str(metricsLR.confusionMatrix().toArray()) +\n",
    "'\\n\\nSummary Of Statistics For Logistic Regression\\n\\nTotal Accuracy  : ' + str(format(metricsLR.accuracy,'.2f')) +\n",
    "'\\n\\nTotal Precision : ' + str(format(metricsLR.precision(),'.2f')) + '\\t Precision 1: ' + str(format(metricsLR.precision(0),'.2f')) + '\\t Precision 2: ' + str(format(metricsLR.precision(1),'.2f')) + '\\t Precision 3: ' + str(format(metricsLR.precision(2),'.2f')) +\n",
    "'\\n\\nTotal Recall    : ' + str(format(metricsLR.recall(),'.2f')) + '\\t Recall 1   : ' + str(format(metricsLR.recall(0),'.2f')) + '\\t Recall 2   : ' + str(format(metricsLR.recall(1),'.2f')) + '\\t Recall 3   : ' + str(format(metricsLR.recall(2),'.2f')) +\n",
    "'\\n\\nTotal fMeasure  : ' + str(format(metricsLR.fMeasure(),'.2f')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5107b9ff9a7b4e35aed04ee187af9416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions using test data:\n",
      "\n",
      "[(1.0, 1.0), (2.0, 2.0), (0.0, 0.0)]"
     ]
    }
   ],
   "source": [
    "## PREDICTIONS USING THE MODEL\n",
    "\n",
    "print('\\nPredictions using test data:\\n')\n",
    "predictionAndLabelsLR_test = test.map(lambda l: (float(modelLR.predict(l.features)), l.label))\n",
    "predictionAndLabelsLR_test.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4f04912ec646acac9916addae7aaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Naive Bayes : \n",
      "\n",
      "[[4. 0. 0.]\n",
      " [1. 2. 1.]\n",
      " [0. 0. 4.]]\n",
      "\n",
      "Summary Of Statistics For Naive Bayes\n",
      "\n",
      "Total Accuracy  : 0.83\n",
      "\n",
      "Total Precision : 0.83\t Precision 1: 0.80\t Precision 2: 1.00\t Precision 3: 0.80\n",
      "\n",
      "Total Recall    : 0.83\t Recall 1   : 1.00\t Recall 2   : 0.50\t Recall 3   : 1.00\n",
      "\n",
      "Total fMeasure  : 0.83"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "\n",
    "                               #   NAIVE BAYES   #                               \n",
    "\n",
    "#################################################################################\n",
    "\n",
    "## TRAINING THE MODEL\n",
    "\n",
    "modelNB = NaiveBayes.train(training)\n",
    "predictionAndLabelsNB = training.map(lambda l: (float(modelNB.predict(l.features)), l.label))\n",
    "metricsNB = MulticlassMetrics(predictionAndLabelsNB)\n",
    "\n",
    "print('Confusion Matrix for Naive Bayes : \\n\\n' + str(metricsNB.confusionMatrix().toArray()) +\n",
    "'\\n\\nSummary Of Statistics For Naive Bayes\\n\\nTotal Accuracy  : ' + str(format(metricsNB.accuracy,'.2f')) +\n",
    "'\\n\\nTotal Precision : ' + str(format(metricsNB.precision(),'.2f')) + '\\t Precision 1: ' + str(format(metricsNB.precision(0),'.2f')) + '\\t Precision 2: ' + str(format(metricsNB.precision(1),'.2f')) + '\\t Precision 3: ' + str(format(metricsNB.precision(2),'.2f')) +\n",
    "'\\n\\nTotal Recall    : ' + str(format(metricsNB.recall(),'.2f')) + '\\t Recall 1   : ' + str(format(metricsNB.recall(0),'.2f')) + '\\t Recall 2   : ' + str(format(metricsNB.recall(1),'.2f')) + '\\t Recall 3   : ' + str(format(metricsNB.recall(2),'.2f')) +\n",
    "'\\n\\nTotal fMeasure  : ' + str(format(metricsNB.fMeasure(),'.2f'))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cf0940277641cd8a02f5ede8aa7ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions using test data:\n",
      "\n",
      "[(1.0, 1.0), (2.0, 2.0), (0.0, 0.0)]"
     ]
    }
   ],
   "source": [
    "## PREDICTIONS USING THE MODEL\n",
    "\n",
    "print(\"\\nPredictions using test data:\\n\")\n",
    "predictionAndLabels_testNB = test.map(lambda l: (float(modelNB.predict(l.features)), l.label))\n",
    "predictionAndLabels_testNB.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################### The End ######################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
